{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery-Spark Integration Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e447026",
   "metadata": {},
   "source": [
    "# PySpark-BigQuery Integration Test\n",
    "\n",
    "This notebook verifies that PySpark can successfully connect to and query Google BigQuery from within your development environment. It demonstrates:\n",
    "\n",
    "1. Creating a SparkSession with the BigQuery connector\n",
    "2. Verifying authentication and project information\n",
    "3. Reading from a public BigQuery dataset\n",
    "4. Reading from your authenticated project's datasets\n",
    "5. Running a simple query through Spark\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Apache Spark 3.x installed (included in this container)\n",
    "- Google Cloud authentication completed (via `gcloud auth login` or service account)\n",
    "- BigQuery connector JAR (referenced via URL in the notebook)\n",
    "- Appropriate BigQuery permissions for datasets you want to access\n",
    "\n",
    "## Packages Required\n",
    "\n",
    "- pyspark\n",
    "- google-cloud-bigquery\n",
    "- pandas\n",
    "- pyarrow (for data conversion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "# -----------------------\n",
    "# Replace these with your actual values\n",
    "\n",
    "# Your GCP dataset and table to query\n",
    "dataset_id = \"my_dataset\"  # Replace with your dataset name\n",
    "table_id = \"my_table\"       # Replace with your table name\n",
    "\n",
    "# A GCS bucket you have read/write access to (for temporary data)\n",
    "bucket_id = \"my_bucket\"  # Replace with your bucket name\n",
    "\n",
    "# Import required libraries\n",
    "import subprocess\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd480f0",
   "metadata": {},
   "source": [
    "## 1. GCP Authentication Information\n",
    "\n",
    "First, let's check which account and project you're currently authenticated with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display GCP authentication information\n",
    "print(\"GCP Authentication Information:\")\n",
    "try:\n",
    "    # Get currently authenticated account\n",
    "    account_cmd = [\"gcloud\", \"config\", \"get-value\", \"account\"]\n",
    "    account = subprocess.check_output(account_cmd).decode().strip()\n",
    "    print(f\"Authenticated as: {account}\")\n",
    "\n",
    "    # Get current project\n",
    "    project_cmd = [\"gcloud\", \"config\", \"get-value\", \"project\"]\n",
    "    project = subprocess.check_output(project_cmd).decode().strip()\n",
    "    print(f\"Current Project: {project}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving authentication info: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3f10a",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark with BigQuery Connector\n",
    "\n",
    "Now we'll create a SparkSession with the BigQuery connector configured. The connector JAR is referenced via URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac50dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session with BigQuery connector\n",
    "print(\"Initializing Spark session with BigQuery connector...\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"BigQuery Integration Test\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-latest_2.12.jar\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Display Spark version for reference\n",
    "spark_version = spark.version\n",
    "print(f\"Using Apache Spark version: {spark_version}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5f04d",
   "metadata": {},
   "source": [
    "## 3. Test with Public Dataset\n",
    "\n",
    "Let's first test the connector using a public dataset that doesn't require special permissions.\n",
    "We'll query the Shakespeare public dataset available in BigQuery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a public dataset (no authentication needed)\n",
    "print(\"Public dataset test:\")\n",
    "try:\n",
    "    # Read Shakespeare public dataset\n",
    "    public_data = (\n",
    "        spark.read.format(\"bigquery\")\n",
    "        .option(\"table\", \"bigquery-public-data:samples.shakespeare\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    # Display schema and sample data\n",
    "    print(\"Schema:\")\n",
    "    public_data.printSchema()\n",
    "\n",
    "    print(\"\\nSample data (5 rows):\")\n",
    "    public_data.show(5, truncate=False)\n",
    "\n",
    "    print(f\"Total rows in public dataset: {public_data.count()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing public dataset: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02219cc",
   "metadata": {},
   "source": [
    "## 4. Test with Your Project's Dataset\n",
    "\n",
    "Now let's try with a dataset in your authenticated project. Make sure you've set the correct values for `dataset_id` and `table_id` at the top of this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with your authenticated project\n",
    "print(\"Authenticated project test:\")\n",
    "\n",
    "try:\n",
    "    # If you have datasets in your project, this will work\n",
    "    your_data = (\n",
    "        spark.read.format(\"bigquery\")\n",
    "        .option(\"table\", f\"{project}.{dataset_id}.{table_id}\")\n",
    "        .option(\"parentProject\", project)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    print(\"Schema of your dataset:\")\n",
    "    your_data.printSchema()\n",
    "\n",
    "    print(\"\\nSample data from your dataset (5 rows):\")\n",
    "    your_data.show(5, truncate=False)\n",
    "\n",
    "    print(f\"Total rows in your dataset: {your_data.count()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing your project's dataset: {e}\")\n",
    "    print(\"\\nIf this failed, check:\")\n",
    "    print(\"1. Your authentication is correct\")\n",
    "    print(\"2. The dataset and table exist in your project\")\n",
    "    print(f\"3. You have permissions to access {project}.{dataset_id}.{table_id}\")\n",
    "    print(\"4. You've updated the notebook with correct dataset_id and table_id values\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33cb4c",
   "metadata": {},
   "source": [
    "## 5. Execute a Simple Query\n",
    "\n",
    "Finally, let's demonstrate executing a simple query directly through the connector.\n",
    "This will use the temporary bucket you specified for results materialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fca72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional test - simple query execution\n",
    "print(\"Executing a simple BigQuery query through Spark:\")\n",
    "try:\n",
    "    # First, verify your dataset exists\n",
    "    print(f\"Using dataset: {dataset_id}\")\n",
    "\n",
    "    # Configure a temporary Cloud Storage bucket (required for the connector)\n",
    "    spark.conf.set(\"temporaryGcsBucket\", bucket_id)\n",
    "\n",
    "    # Execute the query with proper materialization dataset\n",
    "    query_result = (\n",
    "        spark.read.format(\"bigquery\")\n",
    "        .option(\"viewsEnabled\", \"true\")\n",
    "        .option(\n",
    "            \"query\", \"SELECT 'Success!' as message, CURRENT_TIMESTAMP() as timestamp\"\n",
    "        )\n",
    "        .option(\"parentProject\", project)\n",
    "        .option(\n",
    "            \"materializationDataset\", dataset_id\n",
    "        )  # Use this option instead of \"dataset\"\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    query_result.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")\n",
    "\n",
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"\\nSpark session closed. Test complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8fd9b",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "If you encounter errors in this notebook:\n",
    "\n",
    "1. **Authentication Issues**\n",
    "   - Run `!gcloud auth login` in a notebook cell\n",
    "   - Check if your service account has appropriate permissions\n",
    "\n",
    "2. **Dataset/Table Access**\n",
    "   - Verify the dataset and table names are correct\n",
    "   - Ensure you have permissions to access them\n",
    "   - Check if you're using the correct project\n",
    "\n",
    "3. **Temporary Bucket Issues**\n",
    "   - Verify the bucket exists and you have read/write access\n",
    "   - Try creating a new bucket with your project's default service account access\n",
    "\n",
    "4. **Connector Issues**\n",
    "   - Check if the connector JAR is accessible\n",
    "   - Make sure there are no version conflicts with your Spark installation\n",
    "\n",
    "5. **Network Issues**\n",
    "   - Verify your container has internet access\n",
    "   - Check if any firewalls are blocking the connection to GCP\n",
    "\n",
    "For more information on the BigQuery connector for Spark, see the [official documentation](https://github.com/GoogleCloudDataproc/spark-bigquery-connector).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
